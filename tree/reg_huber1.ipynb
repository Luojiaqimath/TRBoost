{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from trb import objectives\n",
    "\n",
    "from trb.learner import TRBLearner\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "n_outliers = 50\n",
    "\n",
    "X, y, coef = datasets.make_regression(\n",
    "    n_samples=n_samples,\n",
    "    n_features=5,\n",
    "    noise=10,\n",
    "    coef=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Add outlier data\n",
    "np.random.seed(0)\n",
    "X[:n_outliers] = 100 + 100 * np.random.normal(size=(n_outliers, 1))\n",
    "y[:n_outliers] = -100 + 100 * np.random.normal(size=n_outliers)\n",
    "\n",
    "\n",
    "# Add 5% strong outliers to the dataset.\n",
    "X_outliers = X[:n_outliers, :]\n",
    "y_outliers = y[:n_outliers]\n",
    "\n",
    "X_inliers = X[n_outliers:, :]\n",
    "y_inliers = y[n_outliers:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gbdt_reg_score(reg, X_test, y_test, n_estimators):\n",
    "    loss = np.zeros((n_estimators,), dtype=np.float64)\n",
    "    score = np.zeros((n_estimators,), dtype=np.float64)\n",
    "    for i, y_pred in enumerate(reg.staged_predict(X_test)):\n",
    "        loss[i] = reg.loss_(y_test, y_pred)\n",
    "        score[i] = r2_score(y_test, y_pred)\n",
    "    return loss, score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRB standard metric: mean=0.8848411850310198, std=0.02818208365748894\n",
      "TRB standard loss: mean=26.456105452274777, std=3.9761579787974295\n",
      "GBDT metric: mean=0.8226221789428994, std=0.024957652935373918\n",
      "GBDT loss: mean=37.872863616655636, std=4.1529142154832295\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./reg_standard_result\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "dataset = 'huber'\n",
    "objective = objectives.Huber()\n",
    "n_estimators = 100\n",
    "\n",
    "trb_best_loss_list = []\n",
    "gbdt_best_loss_list = []\n",
    "\n",
    "trb_best_r2_list = []\n",
    "gbdt_best_r2_list = []\n",
    "\n",
    "trb_best_nestimator_list = []\n",
    "gbdt_best_nestimator_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(i)\n",
    "    seed = np.random.randint(10000, size=1).item()\n",
    "    X_train_in, X_test, y_train_in, y_test = train_test_split(\n",
    "        X_inliers, y_inliers, test_size=0.2, random_state=seed)\n",
    "    X_train = np.vstack((X_train_in, X_outliers))\n",
    "    y_train = np.append(y_train_in, y_outliers)\n",
    "\n",
    "    train_x, eval_x, train_y, eval_y = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # Trb grid search\n",
    "    trb_best_nestimator = n_estimators\n",
    "    trb_best_eval_loss = 1e15\n",
    "    trb_best_alpha = 0.1\n",
    "    trb_best_beta = 10\n",
    "    trb_best_eta = 0\n",
    "    alpha_list = [0.1, 0.5, 1, 5]\n",
    "    beta_list = [0, 1, 10]\n",
    "    eta_list = [0, 0.01, 0.1]\n",
    "    for _alpha in alpha_list:\n",
    "        for _eta in eta_list:\n",
    "            trb_reg = TRBLearner(objective,\n",
    "                                base_score=np.mean(train_y),  # 回归为0\n",
    "                                n_estimators=n_estimators,\n",
    "                                alpha=_alpha,\n",
    "                                eta=_eta)\n",
    "            trb_reg.fit(train_set=(train_x, train_y), eval_set=(eval_x, eval_y))\n",
    "            trb_eval_loss = trb_reg.eval_loss_list\n",
    "            if np.nanmin(trb_eval_loss) < trb_best_eval_loss:\n",
    "                trb_best_nestimator = np.nanargmin(trb_eval_loss)+1\n",
    "                trb_best_alpha = _alpha\n",
    "                trb_best_eta = _eta\n",
    "                trb_best_eval_loss = np.nanmin(trb_eval_loss)\n",
    "\n",
    "    trb_reg = TRBLearner(objective,\n",
    "                        base_score=np.mean(y_train),\n",
    "                        n_estimators=trb_best_nestimator,\n",
    "                        alpha=trb_best_alpha,\n",
    "                        eta=trb_best_eta)\n",
    "    trb_reg.fit(train_set=(X_train, y_train))\n",
    "    y_trb_pred = trb_reg.predict(X_test)\n",
    "    trb_r2 = r2_score(y_test, y_trb_pred) \n",
    "    trb_loss = objective.loss(y_test, y_trb_pred)\n",
    "    trb_best_r2_list.append(trb_r2)   \n",
    "    trb_best_nestimator_list.append(trb_best_nestimator)\n",
    "    trb_best_loss_list.append(trb_loss)\n",
    "\n",
    "\n",
    "    # Saving...\n",
    "    current_path = os.path.join(\n",
    "        folder_path, dataset, f'shuffle_{seed}', 'trb', 'evaluate')\n",
    "    if not os.path.exists(current_path):\n",
    "        os.makedirs(current_path)\n",
    "\n",
    "    np.savez(os.path.join(current_path, \"data.npz\"), \n",
    "        loss = trb_loss,\n",
    "        r2 = trb_r2,\n",
    "        alpha = trb_best_alpha,\n",
    "        eta = trb_best_eta,\n",
    "        n_estiamtors = trb_best_nestimator,\n",
    "    )\n",
    "\n",
    "\n",
    "    # gbdt\n",
    "    gbdt_best_nestimator = n_estimators\n",
    "    gbdt_best_eval_loss = 1e15\n",
    "    gbdt_best_lr = 0.1\n",
    "    lr_list = [0.1, 0.5, 1]\n",
    "    for lr in lr_list:\n",
    "        gbdt_reg = GradientBoostingRegressor(loss='huber', n_estimators=n_estimators,\n",
    "                                            learning_rate=lr,)\n",
    "        gbdt_reg.fit(train_x, train_y)\n",
    "        gbdt_eval_loss = gbdt_reg_score(gbdt_reg, eval_x, eval_y, n_estimators)[0]\n",
    "        if np.nanmin(gbdt_eval_loss) < gbdt_best_eval_loss:\n",
    "            gbdt_best_nestimator = np.nanargmin(gbdt_eval_loss)+1\n",
    "            gbdt_best_lr = lr\n",
    "            gbdt_best_eval_loss = np.nanmin(gbdt_eval_loss)\n",
    "\n",
    "    gbdt_reg = GradientBoostingRegressor(loss='huber', n_estimators=gbdt_best_nestimator,\n",
    "                                        learning_rate=gbdt_best_lr,)\n",
    "    gbdt_reg.fit(X_train, y_train)\n",
    "    y_gbdt_pred = gbdt_reg.predict(X_test)\n",
    "    gbdt_r2 = r2_score(y_test, y_gbdt_pred)\n",
    "    gbdt_loss = objective.loss(y_test, y_gbdt_pred)\n",
    "    gbdt_best_r2_list.append(gbdt_r2)\n",
    "    gbdt_best_nestimator_list.append(gbdt_best_nestimator)\n",
    "    gbdt_best_loss_list.append(gbdt_loss)\n",
    "\n",
    "    # Saving...\n",
    "    current_path = os.path.join(\n",
    "        folder_path, dataset, f'shuffle_{seed}', 'gbdt', 'evaluate')\n",
    "\n",
    "    if not os.path.exists(current_path):\n",
    "        os.makedirs(current_path)\n",
    "\n",
    "    np.savez(os.path.join(current_path, \"data.npz\"), \n",
    "        loss = gbdt_loss,\n",
    "        r2 = gbdt_r2,\n",
    "        lr = gbdt_best_lr,\n",
    "        n_estimators = gbdt_best_nestimator,\n",
    "    )\n",
    "\n",
    "    \n",
    "print('TRB standard metric: mean={}, std={}'.format(np.mean(trb_best_r2_list),\n",
    "                                    np.std(trb_best_r2_list)))\n",
    "print('TRB standard loss: mean={}, std={}'.format(np.mean(trb_best_loss_list),\n",
    "                                    np.std(trb_best_loss_list)))\n",
    "\n",
    "print('GBDT metric: mean={}, std={}'.format(np.mean(gbdt_best_r2_list),\n",
    "                                    np.std(gbdt_best_r2_list)))\n",
    "print('GBDT loss: mean={}, std={}'.format(np.mean(gbdt_best_loss_list),\n",
    "                                    np.std(gbdt_best_loss_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ed999faa5743a97fd1779ad6b235c831720d8033546bba8b0dfe2340e5f08dc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
